{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c054a39fd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    norm = np.array (x / x.max())\n",
    "    return norm\n",
    "    #norm=np.linalg.norm(x)\n",
    "    #if norm==0: \n",
    "    #    return x\n",
    "    #return x/norm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "one_hot_classes = None\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    global one_hot_classes\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    return preprocessing.label_binarize(x,classes=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stddev=0.05\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x=tf.placeholder(tf.float32,(None, image_shape[0], image_shape[1], image_shape[2]), name='x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    height = math.ceil((float(x_tensor.shape[1].value - conv_ksize[0] + 1))/float((conv_strides[0])))\n",
    "    width = math.ceil(float((x_tensor.shape[2].value - conv_ksize[1] + 1))/float((conv_strides[1])))\n",
    "    #height = math.ceil((float(x_tensor.shape[1].value - conv_ksize[0] + 2))/float((conv_strides[0] + 1)))\n",
    "    #width = math.ceil(float((x_tensor.shape[2].value - conv_ksize[1] + 2))/float((conv_strides[1] + 1)))\n",
    "    weight = tf.Variable(tf.truncated_normal((height, width, x_tensor.shape[3].value, conv_num_outputs),stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    maxpool_layer = tf.nn.max_pool(conv_layer, ksize=[1,pool_ksize[0],pool_ksize[1],1], strides=[1,pool_strides[0],pool_strides[1],1], padding='SAME')\n",
    "    return maxpool_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    flattened = x_tensor.shape[1].value * x_tensor.shape[2].value * x_tensor.shape[3].value\n",
    "    return tf.reshape(x_tensor, shape=(-1, flattened)) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs],stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs], dtype=tf.float32))\n",
    "\n",
    "    fc1 = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    out = tf.nn.relu(fc1)\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs],stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs], dtype=tf.float32))\n",
    "    return tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "\n",
    "    stddev=0.01\n",
    "    conv_strides = (2,2) # Getting out of mem errors with stride=1\n",
    "    pool_strides = (2,2)\n",
    "    pool_ksize = (2,2)\n",
    "\n",
    "    conv_num_outputs1 = 32\n",
    "    conv_ksize1 = (2,2)\n",
    "\n",
    "    conv_num_outputs2 = 128\n",
    "    conv_ksize2 = (4,4)\n",
    "\n",
    "    conv_num_outputs3 = 128\n",
    "    conv_ksize3 = (2,2)\n",
    "\n",
    "    fully_conn_out1 = 1024\n",
    "    fully_conn_out2 = 512\n",
    "    fully_conn_out3 = 128\n",
    "\n",
    "    num_outputs = 10\n",
    "    \n",
    "    x = conv2d_maxpool(x, conv_num_outputs1, conv_ksize1, conv_strides, pool_ksize, pool_strides)\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs2, conv_ksize2, conv_strides, pool_ksize, pool_strides)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    #x = conv2d_maxpool(x, conv_num_outputs3, conv_ksize3, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    x = flatten(x)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    x = fully_conn(x,fully_conn_out1)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    x = fully_conn(x,fully_conn_out2)\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #x = fully_conn(x,fully_conn_out3)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    x = output(x, num_outputs)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features[:256],\n",
    "        y: valid_labels[:256],\n",
    "        keep_prob: 1.})\n",
    "    train_acc = session.run (accuracy, feed_dict = {\n",
    "        x: feature_batch, \n",
    "        y: label_batch, \n",
    "        keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Training: {:.6f} Validation: {:.6f}'.format(\n",
    "        loss,\n",
    "        train_acc,\n",
    "        valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2352 Training: 0.224010 Validation: 0.242188\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1216 Training: 0.228960 Validation: 0.203125\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9601 Training: 0.289604 Validation: 0.308594\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.8813 Training: 0.313119 Validation: 0.273438\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.7929 Training: 0.365099 Validation: 0.316406\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.7326 Training: 0.376238 Validation: 0.316406\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.6653 Training: 0.417079 Validation: 0.351562\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.6380 Training: 0.435644 Validation: 0.343750\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.5832 Training: 0.450495 Validation: 0.332031\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.5464 Training: 0.446782 Validation: 0.378906\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.5052 Training: 0.470297 Validation: 0.398438\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.4899 Training: 0.471535 Validation: 0.378906\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.4488 Training: 0.488861 Validation: 0.417969\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.4253 Training: 0.495050 Validation: 0.402344\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.4507 Training: 0.476485 Validation: 0.394531\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.3888 Training: 0.508663 Validation: 0.402344\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.3557 Training: 0.521040 Validation: 0.386719\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.3275 Training: 0.537129 Validation: 0.414062\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.3327 Training: 0.522277 Validation: 0.398438\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.3091 Training: 0.528465 Validation: 0.402344\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.2784 Training: 0.530941 Validation: 0.394531\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.2483 Training: 0.554455 Validation: 0.414062\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.2108 Training: 0.574257 Validation: 0.417969\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.2093 Training: 0.575495 Validation: 0.402344\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.1971 Training: 0.586634 Validation: 0.425781\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.1964 Training: 0.568069 Validation: 0.410156\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.1653 Training: 0.589109 Validation: 0.421875\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.1518 Training: 0.592822 Validation: 0.433594\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.1172 Training: 0.615099 Validation: 0.433594\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.0937 Training: 0.615099 Validation: 0.433594\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.0974 Training: 0.611386 Validation: 0.433594\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.1023 Training: 0.610148 Validation: 0.429688\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.0611 Training: 0.632426 Validation: 0.476562\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.0215 Training: 0.644802 Validation: 0.480469\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.0217 Training: 0.633663 Validation: 0.472656\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.0178 Training: 0.647277 Validation: 0.488281\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.9717 Training: 0.681931 Validation: 0.492188\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.0102 Training: 0.652228 Validation: 0.472656\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.9702 Training: 0.658416 Validation: 0.480469\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.9270 Training: 0.705446 Validation: 0.496094\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.9299 Training: 0.674505 Validation: 0.457031\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.8984 Training: 0.694307 Validation: 0.492188\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.9011 Training: 0.710396 Validation: 0.515625\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.8940 Training: 0.704208 Validation: 0.476562\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.8693 Training: 0.709158 Validation: 0.500000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.8432 Training: 0.727723 Validation: 0.492188\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.8644 Training: 0.701733 Validation: 0.484375\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8729 Training: 0.691832 Validation: 0.511719\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.8537 Training: 0.719059 Validation: 0.480469\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.8346 Training: 0.725248 Validation: 0.472656\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.8328 Training: 0.748762 Validation: 0.523438\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.8344 Training: 0.742574 Validation: 0.539062\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.7827 Training: 0.761139 Validation: 0.507812\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.7830 Training: 0.762376 Validation: 0.488281\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.8260 Training: 0.751238 Validation: 0.503906\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.7825 Training: 0.773515 Validation: 0.531250\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.7296 Training: 0.792079 Validation: 0.500000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.7202 Training: 0.778465 Validation: 0.488281\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.7139 Training: 0.790842 Validation: 0.515625\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.7474 Training: 0.785891 Validation: 0.492188\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.7120 Training: 0.798267 Validation: 0.503906\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.6818 Training: 0.814356 Validation: 0.511719\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.6652 Training: 0.814356 Validation: 0.503906\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.6624 Training: 0.815594 Validation: 0.515625\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.6559 Training: 0.820545 Validation: 0.519531\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.6594 Training: 0.819307 Validation: 0.515625\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.6376 Training: 0.824257 Validation: 0.523438\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.6284 Training: 0.826733 Validation: 0.476562\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.6256 Training: 0.827970 Validation: 0.511719\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.6236 Training: 0.837871 Validation: 0.523438\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.5969 Training: 0.842822 Validation: 0.503906\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.5872 Training: 0.847772 Validation: 0.511719\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.5835 Training: 0.853960 Validation: 0.496094\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.5623 Training: 0.862624 Validation: 0.511719\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.5608 Training: 0.853960 Validation: 0.539062\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.5749 Training: 0.851485 Validation: 0.515625\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.5492 Training: 0.861386 Validation: 0.542969\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.5262 Training: 0.872525 Validation: 0.511719\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.5250 Training: 0.873762 Validation: 0.519531\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.5248 Training: 0.876238 Validation: 0.507812\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.5336 Training: 0.866337 Validation: 0.511719\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.5150 Training: 0.875000 Validation: 0.535156\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.5117 Training: 0.873762 Validation: 0.500000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.5037 Training: 0.868812 Validation: 0.503906\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.5047 Training: 0.879951 Validation: 0.507812\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.5167 Training: 0.877475 Validation: 0.503906\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.4927 Training: 0.875000 Validation: 0.523438\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.4987 Training: 0.881188 Validation: 0.546875\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.4795 Training: 0.888614 Validation: 0.523438\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.4787 Training: 0.887376 Validation: 0.527344\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.4850 Training: 0.889852 Validation: 0.523438\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.4705 Training: 0.902228 Validation: 0.515625\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.4471 Training: 0.904703 Validation: 0.531250\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.4490 Training: 0.891089 Validation: 0.519531\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.4486 Training: 0.894802 Validation: 0.542969\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.4393 Training: 0.910891 Validation: 0.527344\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.4372 Training: 0.904703 Validation: 0.519531\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.4225 Training: 0.912129 Validation: 0.539062\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.4241 Training: 0.907178 Validation: 0.535156\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.4449 Training: 0.896040 Validation: 0.496094\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2712 Training: 0.146040 Validation: 0.125000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.1470 Training: 0.246287 Validation: 0.261719\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.0109 Training: 0.252475 Validation: 0.253906\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.9230 Training: 0.298267 Validation: 0.292969\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.8846 Training: 0.299505 Validation: 0.261719\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.8274 Training: 0.349010 Validation: 0.300781\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.7886 Training: 0.352723 Validation: 0.308594\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.6951 Training: 0.400990 Validation: 0.335938\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.6855 Training: 0.396040 Validation: 0.335938\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.7143 Training: 0.362624 Validation: 0.332031\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.6506 Training: 0.423267 Validation: 0.371094\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.6729 Training: 0.389851 Validation: 0.355469\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.5893 Training: 0.430693 Validation: 0.343750\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.5714 Training: 0.426980 Validation: 0.394531\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.6171 Training: 0.413366 Validation: 0.359375\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.5586 Training: 0.429455 Validation: 0.406250\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.5934 Training: 0.433168 Validation: 0.375000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.4984 Training: 0.446782 Validation: 0.402344\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.4858 Training: 0.459158 Validation: 0.425781\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.5261 Training: 0.456683 Validation: 0.398438\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.4777 Training: 0.452970 Validation: 0.425781\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.5034 Training: 0.457921 Validation: 0.402344\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.4355 Training: 0.474010 Validation: 0.394531\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.4245 Training: 0.487624 Validation: 0.417969\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.4711 Training: 0.466584 Validation: 0.390625\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.4319 Training: 0.481436 Validation: 0.417969\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.4626 Training: 0.465347 Validation: 0.410156\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.3869 Training: 0.497525 Validation: 0.437500\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.3786 Training: 0.517327 Validation: 0.425781\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.4301 Training: 0.496287 Validation: 0.402344\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.3929 Training: 0.500000 Validation: 0.464844\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.4154 Training: 0.481436 Validation: 0.425781\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.3383 Training: 0.521040 Validation: 0.425781\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.3343 Training: 0.532178 Validation: 0.453125\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.3776 Training: 0.511139 Validation: 0.417969\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.3479 Training: 0.513614 Validation: 0.468750\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.3671 Training: 0.506188 Validation: 0.445312\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.3183 Training: 0.545792 Validation: 0.449219\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.3304 Training: 0.528465 Validation: 0.425781\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.3498 Training: 0.524752 Validation: 0.410156\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.3442 Training: 0.513614 Validation: 0.464844\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.3640 Training: 0.525990 Validation: 0.460938\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3014 Training: 0.548267 Validation: 0.425781\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.2888 Training: 0.553218 Validation: 0.453125\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.3140 Training: 0.547030 Validation: 0.433594\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.2998 Training: 0.539604 Validation: 0.472656\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.3224 Training: 0.535891 Validation: 0.476562\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.2789 Training: 0.563119 Validation: 0.425781\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.2642 Training: 0.551980 Validation: 0.488281\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.2936 Training: 0.540842 Validation: 0.449219\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.2619 Training: 0.547030 Validation: 0.476562\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.3269 Training: 0.527228 Validation: 0.453125\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.2628 Training: 0.554455 Validation: 0.453125\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.2262 Training: 0.582921 Validation: 0.464844\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.2607 Training: 0.555693 Validation: 0.441406\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.2360 Training: 0.574257 Validation: 0.468750\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.2856 Training: 0.549505 Validation: 0.460938\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.2274 Training: 0.575495 Validation: 0.464844\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.2022 Training: 0.576733 Validation: 0.480469\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.2294 Training: 0.581683 Validation: 0.464844\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.2190 Training: 0.574257 Validation: 0.496094\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.2808 Training: 0.561881 Validation: 0.488281\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.2082 Training: 0.581683 Validation: 0.468750\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.1987 Training: 0.585396 Validation: 0.464844\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.2214 Training: 0.582921 Validation: 0.457031\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.2112 Training: 0.582921 Validation: 0.492188\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.2631 Training: 0.560644 Validation: 0.484375\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.2143 Training: 0.576733 Validation: 0.464844\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.1847 Training: 0.582921 Validation: 0.480469\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.1940 Training: 0.589109 Validation: 0.460938\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.1985 Training: 0.564356 Validation: 0.472656\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.2753 Training: 0.550743 Validation: 0.449219\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.1996 Training: 0.582921 Validation: 0.492188\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.1626 Training: 0.587871 Validation: 0.484375\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.1989 Training: 0.584158 Validation: 0.464844\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.1808 Training: 0.585396 Validation: 0.488281\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.2113 Training: 0.596535 Validation: 0.472656\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.1837 Training: 0.585396 Validation: 0.468750\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.1404 Training: 0.586634 Validation: 0.476562\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.1646 Training: 0.622525 Validation: 0.476562\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.1490 Training: 0.625000 Validation: 0.484375\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.1899 Training: 0.594059 Validation: 0.480469\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1504 Training: 0.610148 Validation: 0.472656\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.1312 Training: 0.600248 Validation: 0.480469\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.1446 Training: 0.615099 Validation: 0.445312\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.1698 Training: 0.600248 Validation: 0.468750\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.1720 Training: 0.590347 Validation: 0.484375\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.1494 Training: 0.618812 Validation: 0.457031\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.1058 Training: 0.626238 Validation: 0.480469\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.1356 Training: 0.607673 Validation: 0.441406\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1117 Training: 0.611386 Validation: 0.476562\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.1623 Training: 0.610148 Validation: 0.464844\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.1270 Training: 0.606436 Validation: 0.476562\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.0954 Training: 0.642327 Validation: 0.492188\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.1080 Training: 0.629951 Validation: 0.484375\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.1231 Training: 0.602723 Validation: 0.492188\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.1378 Training: 0.616337 Validation: 0.511719\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.1001 Training: 0.627475 Validation: 0.468750\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0733 Training: 0.644802 Validation: 0.500000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.1026 Training: 0.641089 Validation: 0.488281\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.0964 Training: 0.626238 Validation: 0.476562\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.1418 Training: 0.626238 Validation: 0.480469\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.1195 Training: 0.602723 Validation: 0.511719\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.0806 Training: 0.637376 Validation: 0.484375\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.0786 Training: 0.649752 Validation: 0.511719\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.0869 Training: 0.628713 Validation: 0.488281\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.1149 Training: 0.621287 Validation: 0.500000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.0876 Training: 0.623762 Validation: 0.496094\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.0798 Training: 0.638614 Validation: 0.496094\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.0729 Training: 0.649752 Validation: 0.503906\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.0780 Training: 0.622525 Validation: 0.500000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.1225 Training: 0.628713 Validation: 0.496094\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.0763 Training: 0.623762 Validation: 0.523438\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.0509 Training: 0.664604 Validation: 0.500000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.0613 Training: 0.660891 Validation: 0.500000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.0554 Training: 0.638614 Validation: 0.507812\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.1113 Training: 0.613861 Validation: 0.519531\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.0654 Training: 0.643564 Validation: 0.511719\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.0213 Training: 0.672030 Validation: 0.503906\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.0582 Training: 0.644802 Validation: 0.519531\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.0478 Training: 0.639852 Validation: 0.519531\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.0998 Training: 0.629951 Validation: 0.511719\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.0621 Training: 0.634901 Validation: 0.500000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.0458 Training: 0.650990 Validation: 0.503906\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.0319 Training: 0.664604 Validation: 0.492188\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.0341 Training: 0.644802 Validation: 0.503906\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.0916 Training: 0.636139 Validation: 0.507812\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.0776 Training: 0.611386 Validation: 0.464844\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.0245 Training: 0.663366 Validation: 0.503906\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.0367 Training: 0.663366 Validation: 0.523438\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.0265 Training: 0.647277 Validation: 0.515625\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.0603 Training: 0.646040 Validation: 0.492188\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.0382 Training: 0.642327 Validation: 0.511719\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.0146 Training: 0.678218 Validation: 0.488281\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.0126 Training: 0.669554 Validation: 0.542969\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.0037 Training: 0.670792 Validation: 0.542969\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.0395 Training: 0.653465 Validation: 0.507812\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.0068 Training: 0.669554 Validation: 0.519531\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.9882 Training: 0.672030 Validation: 0.500000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.0000 Training: 0.679455 Validation: 0.496094\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.0037 Training: 0.658416 Validation: 0.527344\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.0650 Training: 0.646040 Validation: 0.531250\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.0307 Training: 0.652228 Validation: 0.480469\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.9962 Training: 0.673267 Validation: 0.511719\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.0092 Training: 0.664604 Validation: 0.503906\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.9908 Training: 0.658416 Validation: 0.500000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.0326 Training: 0.670792 Validation: 0.515625\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.0065 Training: 0.659653 Validation: 0.507812\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.9748 Training: 0.683168 Validation: 0.503906\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.9791 Training: 0.688119 Validation: 0.503906\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.9954 Training: 0.660891 Validation: 0.511719\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.0267 Training: 0.660891 Validation: 0.531250\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.9974 Training: 0.667079 Validation: 0.511719\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.9809 Training: 0.678218 Validation: 0.507812\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.9653 Training: 0.691832 Validation: 0.480469\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.9635 Training: 0.672030 Validation: 0.539062\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.9998 Training: 0.680693 Validation: 0.542969\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.9712 Training: 0.690594 Validation: 0.531250\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.9643 Training: 0.701733 Validation: 0.527344\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.9668 Training: 0.691832 Validation: 0.511719\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.9848 Training: 0.663366 Validation: 0.554688\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.9819 Training: 0.683168 Validation: 0.503906\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.9528 Training: 0.688119 Validation: 0.519531\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.9421 Training: 0.704208 Validation: 0.519531\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.9531 Training: 0.694307 Validation: 0.527344\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.9579 Training: 0.676980 Validation: 0.527344\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.9933 Training: 0.684406 Validation: 0.531250\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.9607 Training: 0.683168 Validation: 0.546875\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.9726 Training: 0.683168 Validation: 0.535156\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.9794 Training: 0.701733 Validation: 0.507812\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.9623 Training: 0.670792 Validation: 0.554688\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.9723 Training: 0.693069 Validation: 0.515625\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.9507 Training: 0.683168 Validation: 0.523438\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.9235 Training: 0.705446 Validation: 0.554688\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.9504 Training: 0.690594 Validation: 0.507812\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.9259 Training: 0.681931 Validation: 0.527344\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.9739 Training: 0.690594 Validation: 0.531250\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.9522 Training: 0.680693 Validation: 0.500000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.9350 Training: 0.693069 Validation: 0.531250\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.9478 Training: 0.706683 Validation: 0.519531\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.9207 Training: 0.685644 Validation: 0.523438\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.9549 Training: 0.693069 Validation: 0.535156\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.9390 Training: 0.690594 Validation: 0.523438\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.9182 Training: 0.700495 Validation: 0.531250\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.9251 Training: 0.716584 Validation: 0.531250\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.9209 Training: 0.695545 Validation: 0.535156\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.9559 Training: 0.702970 Validation: 0.546875\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.9489 Training: 0.678218 Validation: 0.527344\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.9092 Training: 0.706683 Validation: 0.535156\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.9105 Training: 0.715347 Validation: 0.542969\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.9073 Training: 0.689356 Validation: 0.511719\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.9446 Training: 0.698020 Validation: 0.531250\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.9139 Training: 0.702970 Validation: 0.531250\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.8958 Training: 0.710396 Validation: 0.535156\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.9044 Training: 0.730198 Validation: 0.531250\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.8843 Training: 0.701733 Validation: 0.523438\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.9273 Training: 0.707921 Validation: 0.554688\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.9316 Training: 0.689356 Validation: 0.546875\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.9046 Training: 0.711634 Validation: 0.550781\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.9038 Training: 0.728960 Validation: 0.539062\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.9049 Training: 0.699257 Validation: 0.542969\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.9319 Training: 0.699257 Validation: 0.519531\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.9136 Training: 0.709158 Validation: 0.519531\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.8803 Training: 0.716584 Validation: 0.558594\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.9080 Training: 0.715347 Validation: 0.531250\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.9133 Training: 0.704208 Validation: 0.546875\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.9261 Training: 0.702970 Validation: 0.527344\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.9042 Training: 0.696782 Validation: 0.542969\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.8718 Training: 0.730198 Validation: 0.566406\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.9085 Training: 0.725248 Validation: 0.550781\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.8727 Training: 0.705446 Validation: 0.558594\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.8968 Training: 0.709158 Validation: 0.550781\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.8827 Training: 0.715347 Validation: 0.546875\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.8675 Training: 0.719059 Validation: 0.546875\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.8879 Training: 0.732673 Validation: 0.546875\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.8749 Training: 0.725248 Validation: 0.558594\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.9006 Training: 0.722772 Validation: 0.539062\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.8961 Training: 0.698020 Validation: 0.550781\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.8604 Training: 0.720297 Validation: 0.566406\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.8545 Training: 0.738861 Validation: 0.554688\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.8774 Training: 0.707921 Validation: 0.593750\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.9055 Training: 0.714109 Validation: 0.570312\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.8943 Training: 0.704208 Validation: 0.542969\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.8796 Training: 0.726485 Validation: 0.539062\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.8579 Training: 0.730198 Validation: 0.558594\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.8571 Training: 0.711634 Validation: 0.562500\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.8954 Training: 0.724010 Validation: 0.554688\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.8982 Training: 0.704208 Validation: 0.531250\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.8606 Training: 0.712871 Validation: 0.585938\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.8718 Training: 0.726485 Validation: 0.550781\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.8598 Training: 0.712871 Validation: 0.546875\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.8893 Training: 0.725248 Validation: 0.554688\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.8986 Training: 0.702970 Validation: 0.535156\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.8501 Training: 0.728960 Validation: 0.574219\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.8412 Training: 0.742574 Validation: 0.519531\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8435 Training: 0.711634 Validation: 0.566406\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.8650 Training: 0.728960 Validation: 0.558594\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.8828 Training: 0.719059 Validation: 0.550781\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.8359 Training: 0.735148 Validation: 0.562500\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.8623 Training: 0.730198 Validation: 0.535156\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.8356 Training: 0.719059 Validation: 0.578125\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.8492 Training: 0.740099 Validation: 0.542969\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.8772 Training: 0.721535 Validation: 0.507812\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.8587 Training: 0.717822 Validation: 0.574219\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.8994 Training: 0.717822 Validation: 0.527344\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.8527 Training: 0.717822 Validation: 0.605469\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.8689 Training: 0.714109 Validation: 0.558594\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.8615 Training: 0.736386 Validation: 0.539062\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.8125 Training: 0.740099 Validation: 0.589844\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.8366 Training: 0.728960 Validation: 0.527344\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.8352 Training: 0.727723 Validation: 0.574219\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.8702 Training: 0.728960 Validation: 0.546875\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.8454 Training: 0.725248 Validation: 0.535156\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.8198 Training: 0.732673 Validation: 0.562500\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.8247 Training: 0.748762 Validation: 0.554688\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.8400 Training: 0.724010 Validation: 0.566406\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.8406 Training: 0.733911 Validation: 0.535156\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.8468 Training: 0.732673 Validation: 0.507812\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.8165 Training: 0.742574 Validation: 0.546875\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.8446 Training: 0.743812 Validation: 0.535156\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.8419 Training: 0.733911 Validation: 0.589844\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.8469 Training: 0.738861 Validation: 0.562500\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.8422 Training: 0.726485 Validation: 0.531250\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.8152 Training: 0.754951 Validation: 0.546875\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.8610 Training: 0.731436 Validation: 0.539062\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.8237 Training: 0.743812 Validation: 0.566406\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.8544 Training: 0.731436 Validation: 0.593750\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.8481 Training: 0.732673 Validation: 0.542969\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.8043 Training: 0.754951 Validation: 0.582031\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.8057 Training: 0.753713 Validation: 0.566406\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.8314 Training: 0.728960 Validation: 0.597656\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.8641 Training: 0.728960 Validation: 0.566406\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.8332 Training: 0.740099 Validation: 0.535156\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.8182 Training: 0.748762 Validation: 0.542969\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.8265 Training: 0.742574 Validation: 0.574219\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.8171 Training: 0.732673 Validation: 0.550781\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.8338 Training: 0.740099 Validation: 0.531250\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.8431 Training: 0.737624 Validation: 0.570312\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.8044 Training: 0.757426 Validation: 0.531250\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.7985 Training: 0.740099 Validation: 0.554688\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.8097 Training: 0.743812 Validation: 0.585938\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.8374 Training: 0.730198 Validation: 0.562500\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.8311 Training: 0.740099 Validation: 0.574219\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.8025 Training: 0.756188 Validation: 0.554688\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.8089 Training: 0.754951 Validation: 0.531250\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.8200 Training: 0.730198 Validation: 0.574219\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.8570 Training: 0.726485 Validation: 0.527344\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.8380 Training: 0.731436 Validation: 0.527344\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.8028 Training: 0.761139 Validation: 0.558594\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.8009 Training: 0.762376 Validation: 0.523438\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.7990 Training: 0.750000 Validation: 0.566406\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.8212 Training: 0.750000 Validation: 0.566406\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.8286 Training: 0.735148 Validation: 0.542969\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.7886 Training: 0.763614 Validation: 0.578125\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.8208 Training: 0.752475 Validation: 0.566406\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.8044 Training: 0.751238 Validation: 0.582031\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.8260 Training: 0.746287 Validation: 0.531250\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.8209 Training: 0.751238 Validation: 0.550781\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.7996 Training: 0.751238 Validation: 0.535156\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.7929 Training: 0.751238 Validation: 0.542969\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.8186 Training: 0.741337 Validation: 0.535156\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.8256 Training: 0.733911 Validation: 0.570312\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.8259 Training: 0.752475 Validation: 0.539062\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.7762 Training: 0.769802 Validation: 0.535156\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.7628 Training: 0.780941 Validation: 0.542969\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.7975 Training: 0.756188 Validation: 0.550781\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.8073 Training: 0.747525 Validation: 0.515625\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.8366 Training: 0.728960 Validation: 0.531250\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.7648 Training: 0.769802 Validation: 0.578125\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.7744 Training: 0.761139 Validation: 0.535156\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.7895 Training: 0.747525 Validation: 0.554688\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.8141 Training: 0.732673 Validation: 0.574219\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.8100 Training: 0.738861 Validation: 0.550781\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.7631 Training: 0.772277 Validation: 0.535156\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.7576 Training: 0.772277 Validation: 0.589844\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.7763 Training: 0.758663 Validation: 0.566406\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.7915 Training: 0.766089 Validation: 0.578125\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.7963 Training: 0.745049 Validation: 0.542969\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.7586 Training: 0.779703 Validation: 0.558594\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.7803 Training: 0.772277 Validation: 0.554688\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.7620 Training: 0.756188 Validation: 0.531250\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.7793 Training: 0.766089 Validation: 0.566406\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.7737 Training: 0.756188 Validation: 0.539062\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.7509 Training: 0.775990 Validation: 0.550781\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.7751 Training: 0.771040 Validation: 0.562500\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.7766 Training: 0.754951 Validation: 0.570312\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.7869 Training: 0.759901 Validation: 0.550781\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.7996 Training: 0.735148 Validation: 0.539062\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.7517 Training: 0.783416 Validation: 0.542969\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.7438 Training: 0.785891 Validation: 0.574219\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.7775 Training: 0.756188 Validation: 0.546875\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.7842 Training: 0.756188 Validation: 0.574219\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.8042 Training: 0.740099 Validation: 0.539062\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.7854 Training: 0.759901 Validation: 0.535156\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.7624 Training: 0.772277 Validation: 0.570312\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.8077 Training: 0.752475 Validation: 0.531250\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.7724 Training: 0.768564 Validation: 0.578125\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.7984 Training: 0.742574 Validation: 0.539062\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.7732 Training: 0.762376 Validation: 0.597656\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.7410 Training: 0.769802 Validation: 0.574219\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.7844 Training: 0.761139 Validation: 0.562500\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.7947 Training: 0.758663 Validation: 0.578125\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.7935 Training: 0.743812 Validation: 0.550781\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.7701 Training: 0.769802 Validation: 0.554688\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.7528 Training: 0.762376 Validation: 0.562500\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.7860 Training: 0.761139 Validation: 0.546875\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.8153 Training: 0.756188 Validation: 0.601562\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.8264 Training: 0.741337 Validation: 0.507812\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.7649 Training: 0.771040 Validation: 0.570312\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.7611 Training: 0.779703 Validation: 0.574219\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.7692 Training: 0.773515 Validation: 0.589844\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.7806 Training: 0.772277 Validation: 0.582031\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.7813 Training: 0.754951 Validation: 0.562500\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.7663 Training: 0.774752 Validation: 0.574219\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.7534 Training: 0.768564 Validation: 0.562500\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.7650 Training: 0.771040 Validation: 0.570312\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.7691 Training: 0.771040 Validation: 0.593750\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.7752 Training: 0.756188 Validation: 0.558594\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.7523 Training: 0.787129 Validation: 0.578125\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.7616 Training: 0.759901 Validation: 0.562500\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.7895 Training: 0.757426 Validation: 0.574219\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.7524 Training: 0.767327 Validation: 0.613281\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.7665 Training: 0.768564 Validation: 0.546875\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.7422 Training: 0.794554 Validation: 0.582031\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.7341 Training: 0.775990 Validation: 0.566406\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7479 Training: 0.767327 Validation: 0.574219\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.7750 Training: 0.764852 Validation: 0.593750\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.7615 Training: 0.761139 Validation: 0.562500\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.7249 Training: 0.798267 Validation: 0.578125\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.7371 Training: 0.780941 Validation: 0.570312\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7323 Training: 0.767327 Validation: 0.597656\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.7387 Training: 0.778465 Validation: 0.589844\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.7453 Training: 0.764852 Validation: 0.550781\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.7028 Training: 0.803218 Validation: 0.625000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.7318 Training: 0.783416 Validation: 0.550781\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7135 Training: 0.782178 Validation: 0.566406\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.7551 Training: 0.767327 Validation: 0.597656\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.7420 Training: 0.764852 Validation: 0.570312\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.7324 Training: 0.795792 Validation: 0.578125\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.7195 Training: 0.788366 Validation: 0.582031\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.7164 Training: 0.780941 Validation: 0.574219\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.7376 Training: 0.777228 Validation: 0.589844\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.7423 Training: 0.758663 Validation: 0.582031\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.7117 Training: 0.788366 Validation: 0.605469\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.7143 Training: 0.792079 Validation: 0.566406\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.7192 Training: 0.783416 Validation: 0.582031\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.7451 Training: 0.773515 Validation: 0.582031\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.7483 Training: 0.774752 Validation: 0.550781\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.6994 Training: 0.800743 Validation: 0.613281\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.7140 Training: 0.790842 Validation: 0.574219\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.7257 Training: 0.773515 Validation: 0.601562\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.7447 Training: 0.778465 Validation: 0.582031\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.7247 Training: 0.774752 Validation: 0.589844\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.6925 Training: 0.795792 Validation: 0.609375\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.7157 Training: 0.804455 Validation: 0.593750\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.7416 Training: 0.777228 Validation: 0.613281\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.7477 Training: 0.769802 Validation: 0.574219\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.7172 Training: 0.780941 Validation: 0.574219\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.6755 Training: 0.803218 Validation: 0.589844\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.6964 Training: 0.785891 Validation: 0.585938\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.7016 Training: 0.789604 Validation: 0.605469\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.7339 Training: 0.778465 Validation: 0.601562\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.7412 Training: 0.768564 Validation: 0.570312\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.6916 Training: 0.797030 Validation: 0.605469\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.7193 Training: 0.794554 Validation: 0.542969\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.7171 Training: 0.782178 Validation: 0.574219\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.7222 Training: 0.790842 Validation: 0.597656\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.7341 Training: 0.767327 Validation: 0.570312\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.7234 Training: 0.778465 Validation: 0.578125\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.6930 Training: 0.794554 Validation: 0.546875\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.7042 Training: 0.785891 Validation: 0.585938\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.7300 Training: 0.780941 Validation: 0.578125\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.7092 Training: 0.775990 Validation: 0.558594\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.7087 Training: 0.793317 Validation: 0.605469\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.7454 Training: 0.783416 Validation: 0.597656\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.6976 Training: 0.809406 Validation: 0.593750\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.7080 Training: 0.793317 Validation: 0.609375\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.7004 Training: 0.784653 Validation: 0.562500\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.6859 Training: 0.792079 Validation: 0.617188\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.7021 Training: 0.799505 Validation: 0.566406\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.7033 Training: 0.801980 Validation: 0.589844\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.7033 Training: 0.792079 Validation: 0.628906\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.6944 Training: 0.795792 Validation: 0.550781\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.6955 Training: 0.790842 Validation: 0.585938\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.7180 Training: 0.805693 Validation: 0.582031\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.7072 Training: 0.792079 Validation: 0.578125\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.7052 Training: 0.795792 Validation: 0.597656\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.6830 Training: 0.793317 Validation: 0.605469\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.6772 Training: 0.810644 Validation: 0.570312\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.7011 Training: 0.804455 Validation: 0.585938\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.7098 Training: 0.795792 Validation: 0.597656\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.7016 Training: 0.795792 Validation: 0.601562\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.6935 Training: 0.794554 Validation: 0.566406\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.6722 Training: 0.793317 Validation: 0.609375\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.6820 Training: 0.814356 Validation: 0.570312\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.7009 Training: 0.788366 Validation: 0.566406\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.6941 Training: 0.792079 Validation: 0.589844\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.6829 Training: 0.785891 Validation: 0.566406\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.6821 Training: 0.805693 Validation: 0.566406\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.6814 Training: 0.805693 Validation: 0.593750\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.7142 Training: 0.779703 Validation: 0.570312\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.6921 Training: 0.792079 Validation: 0.617188\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.6851 Training: 0.789604 Validation: 0.570312\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.6825 Training: 0.806931 Validation: 0.593750\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.6711 Training: 0.815594 Validation: 0.617188\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.6934 Training: 0.790842 Validation: 0.617188\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.6828 Training: 0.818069 Validation: 0.593750\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.6790 Training: 0.785891 Validation: 0.550781\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.6705 Training: 0.799505 Validation: 0.617188\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.6673 Training: 0.813119 Validation: 0.593750\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.6969 Training: 0.798267 Validation: 0.554688\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.6734 Training: 0.814356 Validation: 0.601562\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.6642 Training: 0.789604 Validation: 0.585938\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.6725 Training: 0.801980 Validation: 0.570312\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.6609 Training: 0.806931 Validation: 0.585938\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.6840 Training: 0.794554 Validation: 0.585938\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.6855 Training: 0.783416 Validation: 0.613281\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.6592 Training: 0.798267 Validation: 0.582031\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.6451 Training: 0.821782 Validation: 0.601562\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.6776 Training: 0.818069 Validation: 0.585938\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.6878 Training: 0.794554 Validation: 0.593750\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.6724 Training: 0.808168 Validation: 0.609375\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.6590 Training: 0.798267 Validation: 0.593750\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.6288 Training: 0.832921 Validation: 0.601562\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.6848 Training: 0.815594 Validation: 0.558594\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.6985 Training: 0.784653 Validation: 0.593750\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.6666 Training: 0.814356 Validation: 0.593750\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.6549 Training: 0.783416 Validation: 0.566406\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.6403 Training: 0.809406 Validation: 0.636719\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.6539 Training: 0.823020 Validation: 0.593750\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.6539 Training: 0.810644 Validation: 0.609375\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.6824 Training: 0.800743 Validation: 0.566406\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.6727 Training: 0.795792 Validation: 0.585938\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.6418 Training: 0.810644 Validation: 0.617188\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.6652 Training: 0.820545 Validation: 0.593750\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.6758 Training: 0.794554 Validation: 0.558594\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.6752 Training: 0.792079 Validation: 0.625000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.6570 Training: 0.798267 Validation: 0.605469\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.6446 Training: 0.809406 Validation: 0.589844\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.6509 Training: 0.824257 Validation: 0.550781\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.6608 Training: 0.809406 Validation: 0.582031\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.6513 Training: 0.818069 Validation: 0.582031\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.6493 Training: 0.818069 Validation: 0.601562\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.6262 Training: 0.820545 Validation: 0.613281\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.6576 Training: 0.829208 Validation: 0.570312\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.6419 Training: 0.823020 Validation: 0.542969\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.6549 Training: 0.805693 Validation: 0.585938\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.6460 Training: 0.808168 Validation: 0.589844\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.6265 Training: 0.815594 Validation: 0.578125\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.6518 Training: 0.823020 Validation: 0.617188\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.6516 Training: 0.811881 Validation: 0.613281\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.6513 Training: 0.820545 Validation: 0.601562\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.6416 Training: 0.808168 Validation: 0.605469\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.6125 Training: 0.823020 Validation: 0.605469\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.6237 Training: 0.836634 Validation: 0.597656\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.6440 Training: 0.805693 Validation: 0.609375\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.6430 Training: 0.813119 Validation: 0.593750\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.6283 Training: 0.814356 Validation: 0.585938\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.6084 Training: 0.821782 Validation: 0.578125\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.6291 Training: 0.826733 Validation: 0.578125\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6253980891719745\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d211994a20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
